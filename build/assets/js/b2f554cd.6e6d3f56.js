"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/globally_distributed_webassembly_applications_with_wasmcloud_and_nats","metadata":{"permalink":"/blog/globally_distributed_webassembly_applications_with_wasmcloud_and_nats","source":"@site/blog/globally_distributed_webassembly_applications_with_wasmcloud_and_nats.md","title":"Globally Distributed WebAssembly Applications with wasmCloud and NATS","description":"Taking a wasmCloud lattice from local to globally distributed with NATS and NGS","date":"2022-10-18T13:00:00.000Z","formattedDate":"October 18, 2022","tags":[],"readingTime":12.61,"hasTruncateMarker":true,"authors":[{"name":"Brooks Townsend"}],"frontMatter":{"title":"Globally Distributed WebAssembly Applications with wasmCloud and NATS","image":"/img/ngs-global.png","date":"2022-10-18T13:00:00.000Z","author":"Brooks Townsend","author_profile":"https://linkedin.com/in/brooks-townsend","description":"Taking a wasmCloud lattice from local to globally distributed with NATS and NGS","categories":["webassembly","wasmcloud","nats","distributed","lattice"],"draft":false},"nextItem":{"title":"Using Capabilities to Decouple Non-Functional Requirements","permalink":"/blog/balancing_nfr_coupling"}},"content":"![ngs-global](/img/ngs-global.png)\\n\\nThe first claim we make about wasmCloud on our documentation site is: \\"wasmCloud is a distributed platform...\\" The best definition I could find, on [Wikipedia](https://en.wikipedia.org/wiki/Distributed_computing) of course, is:\\n\\n:::info\\nDistributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system.\\n:::\\n\\n\x3c!--truncate--\x3e\\n\\nSo, by the definition, as soon as we got two WebAssembly modules talking to each other on different networked computers we had a distributed system. Of course, we didn\'t stop there, and today we\'re going to walk through how you can run a global wasmCloud [lattice](https://wasmcloud.dev/reference/lattice/) using NATS and NGS.\\n\\n# How we got to NATS\\n\\nBack in the 0.18.0 days of wasmCloud we supported local host process calls which allowed developers to avoid installing NATS. Now, all wasmCloud hosts run atop NATS as a networking infrastructure that we call a [lattice](https://wasmcloud.dev/reference/lattice/). We\'ve taken a stance _\\"compatible with, but not dependent upon\\"_ for as much as possible (Kubernetes, Docker, bare metal, IoT, nomad) so this is a significant choice, and today you\'ll see the reasons why.\\n\\nWe\'re going to do a brief introduction on the power of NATS, talk about how wasmCloud uses it, and then get into configuring it to connect wasmCloud compute anywhere.\\n\\n# NATS, Leaf Nodes, and NGS\\n\\n[NATS](https://nats.io/) describes itself as \\"Connective Technology for Adaptive Edge & Distributed Systems\\", which does as good of a job as you can to describe such a far-reaching technology. At its base level, NATS enables pub-sub and request-reply messaging on [subjects](https://docs.nats.io/nats-concepts/subjects). You run the NATS server binary, then connect clients to it over a TCP socket [^1] and can publish messages to any other client subscribed on a the same subject. NATS also includes an optional distributed persistence system called [Jetstream](https://docs.nats.io/nats-concepts/jetstream) and a fully-featured authn/authz [security](https://docs.nats.io/nats-concepts/security) system for additional reliability and configuration. There are plenty of features that NATS offers that are out of scope for this guide, but the NATS [documentation](https://docs.nats.io/) is a great place to find those. To sum it up, when you adopt NATS, you dramatically simplify your architecture and the number of tools you need to worry about.\\n\\nA [Leaf Node](https://docs.nats.io/running-a-nats-service/configuration/leafnodes) _extends_ a centralized NATS infrastructure with a local NATS server, allowing you to perform additional authentication steps, route messages locally until they need to be delivered to the central infrastructure. This mechanism is not only efficient, it even allows messages to still flow during network disconnects by continuing to deliver messages locally without an upstream connection. We\'ll be using a Leaf Node today to extend [NGS](https://synadia.com/ngs), the NATS Global Service by [Synadia](https://synadia.com/), which is a NATS supercluster with connection points on the edge, providing low latency worldwide.\\n\\n[^1]: NATS also accepts [connections](https://docs.nats.io/nats-concepts/connectivity) over TLS, WebSockets, and MQTT\\n\\n# wasmCloud and NATS\\n\\nwasmCloud uses NATS in a multitude of ways. To name a few:\\n\\n1.  Request/reply messaging for remote procedure calls between actors and capability providers. Additionally this is used for the [control interface](https://github.com/wasmCloud/control-interface-client/), which allows for remote managing of actors, providers, and hosts via [wash](https://github.com/wasmCloud/wash).\\n2.  Jetstream to persist link definitions and claims so that they are durable and automatically delivered to new wasmCloud hosts joining an existing lattice\\n3.  Communication with a wasmCloud configuration service and the soon-to-come application deployment manager ([wadm](https://github.com/wasmCloud/wadm))\\n\\n# Gathering Prerequisites\\n\\n:::caution\\nThis post was updated on **11 Oct 2022** to include new conveniences like the `wash up` command. This requires at least `v0.12.0` and removes the need to install NATS or the wasmCloud host runtime manually.\\n:::\\n\\nThis example requires a few prerequisites:\\n\\n- The wasmCloud Shell, aka `wash`, from the wasmCloud [installation guide](https://wasmcloud.dev/overview/installation/). If you already have `wash` installed, use `wash --version` to ensure you have version `v0.12.0` or newer\\n- NATS account credentials to access NGS (we\'ll walk through this below)\\n\\nWe\'re also going to use a few files from the aptly named [ngs](https://github.com/wasmCloud/examples/tree/main/ngs) folder in our examples repository to deploy a Wasm microservice (we call this an [actor](https://wasmcloud.dev/reference/host-runtime/actors/)) that securely fetches a random image of a cat or a dog. Later on we\'re going to use some files in this folder, you can either clone this repository or just copy and paste as we go along.\\n\\nTo help illustrate the architecture of our application, take a look at this diagram:\\n\\n![](/img/ngs-global/excalidraw.png)\\n\\nWe\'ll have wasmCloud running both locally and in the cloud (or just on another machine), and we\'ll be spreading compute across these two wasmCloud hosts. Don\'t worry, you won\'t have to look up your local IP address or expose any ports, NATS makes distributed computing a breeze.\\n\\nThe first step will be to get yourself a set of NGS credentials. Navigate to [https://app.ngs.global](https://app.ngs.global) and select \\"Try It Out\\" under **Free**.\\n![](/img/ngs-global/ngs-signup.png)\\n\\nFor simplicity, go ahead and name this account `wasmcloud` to keep it separate from any other NATS accounts you may create in the future.\\n\\n![](/img/ngs-global/ngs-account.png)\\n\\nProceed through the dialogues to sign in through your email until you reach the `Subscription Successful` page (keep in mind this is completely free, and you can\'t accidentally exceed your free tier limits). The last step in this process is to copy the `curl` command with your secret account key to install the NATS CLI and `nsc` to download your NGS credentials\\n\\n![](/img/ngs-global/ngs-curl.png)\\n\\n:::caution\\nThe 58 character key starting with `SA` is a secret (S) key for an account (A). You\'ll want to avoid sharing this value on Twitter or anywhere else public.\\n:::\\n\\nHead back to your terminal and paste in that `curl` command. You\'ll see some output regarding the NATS install process, but all you really need is the last couple of lines:\\n\\n```bash\\nNATS Configuration Context \\"synadia_wasmcloud_default\\"\\n\\n      Description: synadia (Synadia Communications Inc.)\\n      Server URLs: tls://connect.ngs.global\\n      Credentials: /Users/brooks/.nkeys/creds/synadia/wasmcloud/default.creds (OK)\\n             Path: /Users/brooks/.config/nats/context/synadia_wasmcloud_default.json\\n\\nnats-install: All set!\\n```\\n\\nYou can see here where the `Credentials` file is stored, that\'s what you\'ll use to authenticate to NGS. Keep this path in mind as we\'ll come back to it. You can do a quick request through the `nats` CLI to see that it\'s all working:\\n\\n```bash\\n$ nats req ngs.echo \'Anyone out there?\'\\n11:30:10 Sending request on \\"ngs.echo\\"\\n11:30:10 Received with rtt 21.005416ms\\n[Ohio, US]: \\"Anyone out there?\\"\\n```\\n\\n# Running DogsAndCats on your Local Machine\\n\\nNow that you have walked through the wasmCloud installation guide and have valid NGS credentials, we\'re ready to take wasmCloud global!\\n\\nTo start, let\'s get NATS, wasmCloud, and the DogsAndCats example running on our local machine. This is what you can think of as the local development setup for wasmCloud but instead of using a standalone NATS server we\'ll be using a leaf node that connects to NGS.\\n\\nGo ahead and locate the `Credentials` file that you saw in the NGS install output to that directory as well. If you named your account `wasmcloud` like above, then it will be located under a folder called `.nkeys` in your **HOME** directory.\\n\\n```bash\\ncat ~/.nkeys/creds/synadia/wasmcloud/default.creds\\n-----BEGIN NATS USER JWT-----\\neyJ0...elided\\n------END NATS USER JWT------\\n\\n************************* IMPORTANT *************************\\nNKEY Seed printed below can be used to sign and prove identity.\\nNKEYs are sensitive and should be treated as secrets.\\n\\n-----BEGIN USER NKEY SEED-----\\nSU...elided\\n------END USER NKEY SEED------\\n\\n*************************************************************\\n```\\n\\nHere, we\'ll use `wash up` to launch a NATS leaf node connected over TLS to NGS and a wasmCloud host connected to that leaf node. We\'re going to use the account credentials you just generated to authenticate, and the wasmCloud logs will print directly to the terminal. To do so, we can specify the NGS address as the remote URL and your account credentials as the NATS credsfile:\\n\\n```bash\\nwash up --nats-remote-url tls://connect.ngs.global --nats-credsfile ~/.nkeys/creds/synadia/wasmcloud/default.creds\\n```\\n\\nYou should see output like the following:\\n\\n```plain\\n\ud83c\udfc3 Running in interactive mode, your host is running at http://localhost:4000\\n\ud83d\udeaa Press `CTRL+c` at any time to exit\\n11:37:41.559 [info] Wrote \\"./host_config.json\\"\\n11:37:41.560 [info] Wrote \\"/Users/brooks/.wash/host_config.json\\"\\n11:37:41.560 [info] Connecting to control interface NATS without authentication\\n11:37:41.560 [info] Connecting to lattice rpc NATS without authentication\\n11:37:41.560 [info] Host NATWVDH3WZHYQQG3GFXPGRW5IAF5O4YCTMZ66LIM2WRSIVRDOTMIWN5Y (morning-moon-3881) started.\\n11:37:41.560 [info] Valid cluster signers: CD263EGQIG4DKCZAYV6ZMDAX3LOV4PKEGNCZF2THIYONC7W4CAXELVZZ\\n11:37:41.560 [warning] WARNING. You are using an ad hoc generated cluster seed.\\nFor any other host or CLI tool to communicate with this host,\\nyou MUST copy the following seed key and use it as the value\\nof the WASMCLOUD_CLUSTER_SEED environment variable:\\n\\nSCAGI4US72YWQT6TAJBCK77XSOAJMWI5PCG5MICC3FWNRFQLHE53BEZIFU\\n\\nYou must also ensure the following cluster signer is in the list of valid\\nsigners for any new host you start:\\n\\nCD263EGQIG4DKCZAYV6ZMDAX3LOV4PKEGNCZF2THIYONC7W4CAXELVZZ\\n\\n\\n11:37:41.564 [info] Started wasmCloud OTP Host Runtime\\n11:37:41.566 [info] Running WasmcloudHostWeb.Endpoint with cowboy 2.9.0 at 0.0.0.0:4000 (http)\\n11:37:41.567 [info] Access WasmcloudHostWeb.Endpoint at http://localhost:4000\\n11:37:41.665 [info] Lattice cache stream created or verified as existing (0 consumers).\\n11:37:41.665 [info] Attempting to create ephemeral consumer (cache loader)\\n11:37:41.667 [info] Created ephemeral consumer for lattice cache loader\\n```\\n\\nFrom this command output, go ahead and save your 58 character cluster seed starting with **SC** (it will be different than the seed in the sample output above). We\'re going to use it again later.\\n\\nYou now have a running wasmCloud host using NGS as the lattice infrastructure! Even though we\'re connected to NGS, our leaf node is smart enough to always route traffic locally if possible, saving the overhead of a remote network hop. This is key for enabling wasmCloud to function even if you lose connectivity to NGS briefly, all actors and providers on a host will continue as if nothing happened.\\n\\nWe can continue by deploying our ngs application from the examples repository which consists of: our DogsAndCats actor, a capability provider that implements the `wasmcloud:httpserver` contract and a capability provider that implements the `wasmcloud:httpclient` contract, both of which we provide as wasmCloud first-party providers but could be swapped to any other implementation at runtime.\\n\\n```bash\\nwash ctl start actor wasmcloud.azurecr.io/dogs-and-cats:0.1.0\\nwash ctl link put MCUCZ7KMLQBRRWAREIBQKTJ64MMQ5YKEGTCRGPPV47N4R72W2SU3EYMU VAG3QITQQ2ODAOWB5TTQSDJ53XK3SHBEIFNK4AYJ5RKAX2UNSCAPHA5M wasmcloud:httpserver ADDRESS=0.0.0.0:8081\\nwash ctl link put MCUCZ7KMLQBRRWAREIBQKTJ64MMQ5YKEGTCRGPPV47N4R72W2SU3EYMU VCCVLH4XWGI3SGARFNYKYT2A32SUYA2KVAIV2U2Q34DQA7WWJPFRKIKM wasmcloud:httpclient\\nwash ctl start provider wasmcloud.azurecr.io/httpserver:0.15.0\\nwash ctl start provider wasmcloud.azurecr.io/httpclient:0.4.0\\n```\\n\\nOnce everything completes, check out a pet picture at [http://127.0.0.1:8081](http://127.0.0.1:8081)! You can refresh to your heart\'s desire to see pictures of cats and dogs, and you\'ve deployed your application on wasmCloud.\\n\\n[^2]: When using Jetstream domains, NATS maps some internally used topics to use the topic specific to that Jetstream domain. At the wasmCloud level, this would enable you to reuse the same NATS infrastructure on completely different domains and lattice prefixes for multi-tenancy, but it\'s not necessary to know these details for today\'s example.\\n\\n# Turning the Knob from Local to Global\\n\\nFor this step you\'re going to need another computer. This can be a Cloud VM, a Docker container, or even your friends laptop. The architecture can be x86_64 or aarch64, and the operating system can be Macos, Windows, or Linux. The instructions are all the same regardless of your choice (thanks WebAssembly!) For today, I chose to do this on a Google Cloud Platform e2 micro instance which is included in their [free](https://cloud.google.com/free) tier.\\n\\nYou\'ll want to get terminal access to to your new machine and then follow the wasmCloud [installation guide](https://wasmcloud.dev/overview/installation/) `wash` as you did before. You\'ll need a few pieces of information to ensure this host can properly join your lattice:\\n\\n### **default.creds**\\n\\nThis is the same set of credentials you used in the previous step, located under `~/.nkeys/creds/synadia/wasmcloud/default.creds` if your account name is `wasmcloud`. The values contained inside of the credsfile can be supplied manually to connect, but for simplicity we recommend just copying it over to your second machine.\\n\\n### **Cluster Seed**\\n\\nwasmCloud uses a cluster seed to sign and verify each invocation (e.g. `HttpServer.HandleRequest`) in wasmCloud. This is a part of wasmCloud\'s [zero-trust security model](https://wasmcloud.dev/app-dev/secure/clusterkeys/) and any invocations that aren\'t signed with a verified issuer (e.g. from an unknown host) will be denied before it even reaches the actor /\xa0provider. This can be found in the output of your `wash up` command that you ran earlier as the 58 character seed starting with **SC**.\\n\\nOther than that, both hosts will use the default values from `wash up` like the JetStream domain (core) and lattice prefix (default). You can run the following command to launch your second host, which specifies a label, the cluster seed, and a separate JetStream domain for the _leaf node_ to allow your locally launched host to remain as the \\"primary\\" node:\\n\\n```bash\\nHOST_machine=second wash up --nats-remote-url tls://connect.ngs.global --nats-credsfile ./default.creds --cluster-seed SCAGI4US72YWQT6TAJBCK77XSOAJMWI5PCG5MICC3FWNRFQLHE53BEZIFU --nats-js-domain extender --wasmcloud-js-domain core\\n```\\n\\nYou should see a similar dump of logs, but notably you should see that you are connecting to a stream with one consumer (your local machine)\\n\\n```plain\\n16:06:22.896 [info] Lattice cache stream created or verified as existing (1 consumers).\\n```\\n\\nAnd now, on your local machine, check out [http://localhost:4000](http://localhost:4000). You should see your DogsAndCats resources and additionally under your **Host Info** section you\'ll see two hosts:\\n\\n![](/img/ngs-global/dashboard.png)\\n\\nWe can go ahead and schedule a few extra replicas of the DogsAndCats actor on the cloud host and an HTTPClient provider using `wash` or by using the dashboard. These commands can be run with `wash` on either machine, the result is the same!\\n\\n```bash\\n# The constraint flag ensures we start on a host with that label\\nwash ctl start actor wasmcloud.azurecr.io/dogs-and-cats:0.1.0 --constraint machine=second\\nwash ctl start provider wasmcloud.azurecr.io/httpclient:0.4.0 --constraint machine=second\\n```\\n\\nWe\'ve now transformed this app from running as a monolith to running distributed across two machines, with resources running both local and in the cloud. You can even remove the DogsAndCats actor from your local machine and everything will immediately failover to the cloud.\\n\\n# Wrapping up\\n\\nIn this guide we used NGS and NATS Leaf Nodes to connect two wasmCloud hosts; one running locally and one running in the cloud. This guide demonstrated how you can run multiple instances of actors and different capability providers _anywhere_ and how you, the developer, don\'t need to change your business logic to make this happen. We\'re all about making our developer experience world-class, and that means zero code changes from local development to running across different clouds with highly distributed infrastructure. To drive this home, here\'s few things that you didn\'t have to deal with today: IP addresses, security group rules, load balancing requests, failover logic, NATS cluster setup, configuring TLS communications, and of course recompiling for different architectures /\xa0operating systems. All of those are taken care of by the NATS and wasmCloud.\\n\\nWe used the NGS free tier to simplify the infrastructure setup, though it\'s worth noting that NGS is not a required component of this architecture. You can replace NGS with any NATS [cluster](https://docs.nats.io/running-a-nats-service/configuration/clustering) and the result is the same, there\'s no required cost to connect more than two hosts together.\\n\\nIf you\'d like to see the next level of this NGS + Leaf Node setup with wasmCloud, check out [Disrupting the Downtime Continuum](https://www.youtube.com/watch?v=wjwKmq16shI) the talk Taylor and I gave last KubeCon EU where we used these instructions with one more leaf node and demonstrated live fail over between clouds with wasmCloud.\\n\\nWe\'re looking forward to seeing what you can do with this guide! If you give this a try and do something awesome or need any assistance, join our community [Slack](https://slack.wasmcloud.com/) or open an issue on our wasmCloud [repository](https://github.com/wasmCloud/wasmCloud)."},{"id":"/balancing_nfr_coupling","metadata":{"permalink":"/blog/balancing_nfr_coupling","source":"@site/blog/balancing_nfr_coupling.md","title":"Using Capabilities to Decouple Non-Functional Requirements","description":"A look at the motivation and design behind loosely coupling services for actors","date":"2022-10-04T13:00:00.000Z","formattedDate":"October 4, 2022","tags":[],"readingTime":6.275,"hasTruncateMarker":true,"authors":[{"name":"Kevin Hoffman"}],"frontMatter":{"title":"Using Capabilities to Decouple Non-Functional Requirements","image":"/img/train_coupling.jpg","date":"2022-10-04T13:00:00.000Z","author":"Kevin Hoffman","author_profile":"https://www.linkedin.com/in/%F0%9F%A6%80-kevin-hoffman-9252669/","description":"A look at the motivation and design behind loosely coupling services for actors","categories":["webassembly","wasmcloud","nfr","capabilities","design"],"draft":false},"prevItem":{"title":"Globally Distributed WebAssembly Applications with wasmCloud and NATS","permalink":"/blog/globally_distributed_webassembly_applications_with_wasmcloud_and_nats"},"nextItem":{"title":"WebAssembly and the Road to Ubiquity","permalink":"/blog/road_to_ubiquity"}},"content":"![train-coupling](/img/train_coupling.jpg)\\n\\nThis post explores the motivation and design behind the loose coupling between our actors and capabilities.\\n\\nDevelopers have a number of creeds to which we hold dear. Sometimes these show up as pattern and practice recommendations. Sometimes they appear in blog posts, conference talks, or streams. They can also appear indirectly via the code we write. One such creed is the tenet of _loose coupling_. Everything needs to be loosely coupled, because we all known and preach that tight coupling is objectively bad. Everyone seems to know this, but we rarely stop to think about _why_.\\n\\n\x3c!--truncate--\x3e\\n\\nBefore getting into the meat of this post, let\'s take a second to define **non-functional requirements**, often abbreviated as **NFR**s. The core, so-called \\"pure\\" functional part of our business logic defines the **_what_** of the application, while the NFRs define the **_how_**.\\n\\nBy way of example, let\'s examine the anatomy of a fairly commonplace microservice that you might see deployed in any enterprise in a cloud or on Kubernetes. This example service is a RESTful one that validates requests, reads and write bank account information from a data store, and responds accordingly with JSON payloads over HTTP. For this sample, we might have the following sets of functional and non-functional requirements.\\n\\n**Functional Requirements**:\\n\\n- Allow for the creation, deletion, and update of a bank account\\n- Allow for querying summaries of multiple accounts\\n- Allow for querying details on a single account\\n- Validate all requests according to business rules\\n- Accept requests via HTTP\\n\\n**Non-Functional Requirements**:\\n\\n- Store account details information in Postgres\\n- Store account summaries in an in-memory Redis cache\\n- Emit logs to stdout\\n- Run 2 instances of the service per availability zone\\n- Secure access to the HTTP endpoint via reverse proxy\\n- Emit open telemetry spans and traces\\n- Support and expose a prometheus metrics endpoint\\n- Use _(your favorite)_ HTTP server library\\n- Use _(your favorite)_ SSL tools\\n- Secure access to the service via bearer token\\n\\nThe amount of work required is definitely skewed toward the NFR end of the scale. With most of today\'s traditional development tools and ecosystems, every deployment artifact bundles the solution to functional requirements with _just one_ of the many solutions to the non-functional requirements _at build time_.\\n\\nIt\'s pretty routine for developers to build a service that only works with Redis, only works with Postgres, only supports one way of emitting traces and spans, only manages logs one way, only handles prometheus metrics one way, only secures things one way, ... _you get the idea_. We might have some clever abstractions in our library code, we might even deftly employ [anti-corruption layers](https://learn.microsoft.com/en-us/azure/architecture/patterns/anti-corruption-layer), but in the end, _we own our dependencies_, and despite calling it a microservice, we\'re shipping a brittle monolith that only works against one rigid target environment.\\n\\nLike everything in our line of work, there are _tradeoffs_. There are some advantages to making fixed architecture decisions once and codifying those decisions into the deployment artifact. This can often buy you rapid time to market and simplicity in some areas, while dramatically increasing complexity in others. The reverse is true as well, if everything is too flexible, too loosely coupled, then this mess may be too difficult to manage.\\n\\n![Coupling Spectrum](/img/coupling_spectrum.png)\\n\\nAs illustrated, there is a spectrum of coupling from utter and total tight coupling to free and unfettered, uncoupled design. On the far right, all of our components are stuck together, then crammed into a box and sealed. On the far left, we can end up with thousands of little tiny pieces that are so flexible that maintenance and discovery is a nightmare. Without me saying it, I bet you can imagine examples of current technologies that sit on either end of this spectrum.\\n\\nWhat we want, rather than extremes, is the _right_ balance. I want to write _just_ the code that satisfies my functional requirements, and then let other aspects of my system take care of satisfying the NFRs. To satisfy the above example, I want to write code that utilizes _some_ key-value store, _some_ SQL database, _some_ web server. I can then run that component against a test environment on my laptop, a medium-sized environment in the CI tests, and then massive scale when in production. More importantly, _while in production_, I can even change the deployment shape to suit changes in my user base, demand, load, etc.\\n\\nWhen we separate the functional and non-functional requirements across specific [contracts](https://en.wikipedia.org/wiki/Design_by_contract), then we can hopefully achieve the right balance between coupling. You might look at the word _contract_ here and think we\'re just going back to anti-corruption layers or abstraction libraries inside our code. The difference is subtle but vital: requiring that the thing that satisfies the contract _not be a part of your code_ is the magic sauce that balances the coupling scales.\\n\\nIf we own our dependencies, and the implementation of a contract is _not_ a tightly coupled dependency, then we can finally say that, as developers, we don\'t own the non-functional requirements. NFRs are still mandatory, still important, and still have to be dealt with. However, if our _unit of deployment_ doesn\'t own them, then they are free to change, update, and move on their own cadence, _for their own reasons_.\\n\\n## wasmCloud, NFRs, and Capabilities\\n\\nAll of this lead-up and background hopefully explains why wasmCloud has chosen the abstractions we\'ve chosen. We know that if we write a microservice that requires access to an **S3** bucket, it\'s going to be a huge pain to work with locally (even with tools like _minio_). The functional requirements of my code are \\"store this data in a blob store\\". Why should we be forced to decide on S3 and then live with that decision for the lifetime of the component?\\n\\nBy implementing a contract like the one we have for `wasmcloud:blobstore`, and writing code against that contract, wasmCloud actors can interact with files on disk during the local developer iteration cycle, some other abstraction during automated tests, and then real S3 buckets in production. The important thing is that the code no longer owns the choice of S3, or the configuration thereof.\\n\\nWebAssembly\'s enforced hard boundary between the guest and the host mandates that developers choose where their code ends and the host-provided functionality begins. By forcing us to draw this line for real rather than have this line be something that is either implicitly or accidentally drawn (and subsequently violated), we can actually turn this so-called limitation into an advantage and build highly composable, distributed components that have _just the right_ amount of coupling with the components that satisfy their non-functional requirements.\\n\\nTo see how wasmCloud has leveraged this kind of NFR decoupling, check out our documentation on [capability providers](https://wasmcloud.dev/app-dev/std-caps/). For an example of us demonstrating hot-swapping between a file system and S3 live, at runtime, without a recompile or redeploy, check out this [wasmCloud community meeting](https://youtu.be/s_Y-ISP58qk?t=522).\\n\\nToday wasmCloud uses the Smithy DSL to define and describe these contracts. When we started, we had to fend for ourselves when it came to contracts and code generation. Hopefully soon, when the [component model](/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future.md) becomes more of a real thing, we\'ll be able to take our contracts and turn them into component specifications that will work in wasmCloud _or anywhere else_ that supports components.\\n\\n**_That_** is the real pot of gold at the end of the rainbow, and the ultimate payoff for maintaining the right coupling balance.\\n\\n---\\n\\n_Cover Photo by Daniel Schwen - Own work, CC BY-SA 4.0, [https://commons.wikimedia.org/w/index.php?curid=4784168](https://commons.wikimedia.org/w/index.php?curid=4784168)_"},{"id":"/road_to_ubiquity","metadata":{"permalink":"/blog/road_to_ubiquity","source":"@site/blog/road_to_ubiquity.md","title":"WebAssembly and the Road to Ubiquity","description":"A brief look down WebAssembly\'s road to ubiquity","date":"2022-09-19T13:00:00.000Z","formattedDate":"September 19, 2022","tags":[],"readingTime":7.905,"hasTruncateMarker":true,"authors":[{"name":"Kevin Hoffman"}],"frontMatter":{"title":"WebAssembly and the Road to Ubiquity","image":"/img/ij_map_sample.jpeg","date":"2022-09-19T13:00:00.000Z","author":"Kevin Hoffman","author_profile":"https://www.linkedin.com/in/%F0%9F%A6%80-kevin-hoffman-9252669/","description":"A brief look down WebAssembly\'s road to ubiquity","categories":["webassembly","wasmcloud","ubiquity","wasi","component model"],"draft":false},"prevItem":{"title":"Using Capabilities to Decouple Non-Functional Requirements","permalink":"/blog/balancing_nfr_coupling"},"nextItem":{"title":"Reflections on Three Years of wasmCloud","permalink":"/blog/wasmcloud_third_anniversary"}},"content":"![sample-map](/img/ij_map_sample.jpeg)\\n\\nIt may seem odd or counter-intuitive, but most of us within the WebAssembly community are eagerly awaiting the day that WebAssembly becomes _\\"boring\\"_. Choosing so-called boring technology is a good, safe bet for building production systems. Boring technology does what it\'s supposed to, it\'s easy to work with, it doesn\'t crash or break down, and has a simple developer experience. This is what we want WebAssembly to be: boring and **_ubiquitous_**.\\n\\n\x3c!--truncate--\x3e\\n\\nUnfortunately, we\'re not there yet. WebAssembly is far from boring these days. Building a `.wasm` module involves a lot of bespoke tooling, knowledge (much of which is tribal or hard to discover), plugins, and extensions. The experience varies between languages, and, much to our chagrin, even operating systems. Worse, the experience also varies whether you\'re targeting a browser or not, despite our protests that the browser should never be treated as a \\"more equal\\" citizen in the Wasm community. Working in the WebAssembly ecosystem today involves a number of high-caliber footguns.\\n\\nAs we move forward along the road to ubiquity, speed bumps and friction points will be smoothed out and the high-drama of today will become the \\"boring\\" of tomorrow.\\n\\nIn the not-so-distant future, WebAssembly will cease to become an end goal and will instead become a simple implementation detail; a mere checkbox or command-line flag. Developers using Apple\'s Xcode tools will create a project and check a box indicating that they\'re targeting the build as a WebAssembly Component. VScode and other IDEs will have integrated support for components that expose their interfaces through the component model\'s `.wit` (or whatever it becomes next) definitions, so people will get full syntax highlighting and type checking when they create a component that relies upon another component. There will be dependency visualization tools showing the chain of required components. We will have public repositories, artifactory, and other repository vendors will all natively support storing, querying, and annotating WebAssembly modules.\\n\\nDevelopers using wasmCloud or Cosmonic will simply choose features from the SDK, compile, and deploy\u2014all while blissfully unaware of how much WebAssembly contributed to that flow.\\n\\nIn this (hopefully near) future, developers will have the luxury of choice at multiple levels: the _engine_ level, the _specification_ level, and the _application_ level.\\n\\n## Engine-Level Choice\\n\\nAt the **engine** level, as the name implies, developers can choose which low-level engine they want to use for their `wasm` modules. Think of this like picking the right tool from the tool chest; optimizing the choice of engine to your particular needs.\\n\\nThis engine could be optimized and focused on any number of targets or categories like small devices, the cloud, a browser, or other bespoke environments with highly tuned characteristics. In keeping with WebAssembly\'s portability promise, the choice of engine should never require a refactor, redesign, or even a recompilation.\\n\\nIn the current state of the world, we can (for the most part) pick and choose which engine we want based on features or size or performance optimizations. If we want to explore features from standards that haven\'t yet been ratified, then we\'ll want to use [wasmtime](https://wasmtime.dev/).\\n\\n## Specification-Level Choice\\n\\nIt seems likely that ultimately WASI will not be a single all-encompassing walled box of extras that developers get to use with their modules, but rather a grab-bag of opt-in choices to multiple standards like networking, cryptography, file system and OS access, video cards/GPUs, etc. It\'s also pretty likely that these opt-in specifications will be defined as components using the component model.\\n\\nDevelopers wishing to take advantage of specific WASI features are going to have to make that choice at design time and this choice gets baked into their WebAssembly module. If a developer chooses to write to stdout via WebAssembly, this may require one component while publishing a message over a message broker topic would be yet another component.\\n\\nWASI is also likely going to be used as a gap bridging device. People who want the portability of WebAssembly, have little interest in the component model, and still want their big legacy libraries to compile and work[^1] properly will leverage WASI-aware compilers/linkers (like the way Rust/LLVM is today) to enjoy the best of both worlds. The ecosystem will likely have multiple \\"smoke and mirror\\" tools that highly leverage WASI in clever ways to further provide \\"magic\\" and \\"hand-waving\\" to hide complexity from developers and operations. Unfortunately, many of these fancy shims will probably still require a browser.\\n\\nAt the **engine** and **specification** level, there should be no issues around vendor lock-in and forcing developers to choose between clouds, platforms, environments, etc. People will be able to choose the engine they need based on their requirements and pick and choose which component or WASI level they need based on requirements, not the artificial limitations we have today.\\n\\n## Application-Level Choice\\n\\nToday, the choice of application or application platform to support WebAssembly modules is extremely limited. In addition to the lack of choice variety, each choice requires a _total embrace_ and **_lock_** into a particular vendor\'s environment, SDK, tool chain, and so on.\\n\\nIn the current ecosystem, if you want edge functions written in WebAssembly, you\'ll have to use _Fastly_ or _CloudFlare_\'s SDK. If you want to add third-party plugins to your application, you\'d use _E2 Core_\'s SDK. If you want self-contained, freestanding microservices written in WebAssembly, you could use the _Spin_ SDK. If you want portable distributed actors loosely coupled to capability contracts, you may want to use the _wasmCloud_ SDK.\\n\\n## Embracing the Road to Ubiquity\\n\\nAs the WebAssembly community and ecosystem moves forward, we plan on continuing to move at the vanguard of this momentum, so that we\'re adapting our foundations to incorporate new power, features, and flexibility as it becomes available and more mature.\\n\\n### The Developer SDK\\n\\nThe wasmCloud developer experience today is very much influenced by the notion that we originally wanted there to be no measurable difference between consuming [first party capability providers](https://wasmcloud.dev/app-dev/std-caps/) and consuming third party providers.\\n\\nHowever, since everyone making an application or platform decision at this level will be opting into the vendor\'s SDK, it is necessary for us to make this SDK as optimized for developers as as possible, making the following first-party activities and capabilities \\"brain-dead simple\\" _by default_:\\n\\n- HTTP Server\\n- HTTP Client\\n- Message Broker\\n- Blob Store (e.g. S3)\\n- Key Value\\n- SQL DB\\n- Actor-to-Actor Calls\\n\\nThis wasmCloud developer SDK will be a thin wrapper on top of the generated interface code designed to smooth and optimize the developer experience and ease of use. Using this SDK will insulate developers from the churn and change happening at the engine and specification level. As `wit` and the component model mature, we\'ll simply integrate that work into our SDK, sparing developers from as many breaking changes as possible.\\n\\n### The Component Model\\n\\nOnce WebAssembly is boring and ubiquitous and tooling exists for all manner of scenarios and the component model has become mature, developers will be able to rapidly adapt their existing code bases to any number of platform providers because consuming them becomes just a matter of generating client code against wit models.\\n\\nIt\'s worth noting that practically speaking, most developers won\'t be doing this. The \\"easy default\\" case will likely be that developers will choose to use the ready-made, shrink-wrapped SDK of whichever platform they choose to run their code on.\\n\\nHowever, being built on the [component model](https://wasmcloud.com/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future/) gives us the ability to rapidly adapt to any changes in the community and, more importantly, assert that there is _no vendor lock-in for consuming wasmCloud-based services like Cosmonic_, only the choice of relying on any of the portable component modules that represent the developer-facing contract for capability providers.\\n\\nThere is an effort within the standards community to describe a means by which a wasm component can specify the requirements of its host in terms of the components it uses. At the moment, these specifications are called _world files_, but that nomenclature could change at any moment.\\n\\nThe concept of this type of requirements specification is subtle yet _incredibly_ powerful. In a hypothetical future world where WebAssembly and the component model are both boring, a module running in \\"vendor A\\"\'s cloud that requires a key-value store could be moved to \\"vendor B\\"\'s cloud without design change or recompilation, provided both vendors support exposing the WASI key-value component in their \\"world\\". This could truly be a fulfillment of the portability promise beyond just escaping from the confines of OS and CPU coupling.\\n\\nIn this bright, idyllic, near-future world[^2], people write code _one way_ and then the selection of where they deploy their code becomes one of personal preference, financial concerns, and available vendor features. No longer will we be locked into a particular vendor because that\'s the SDK we started experimenting with a year ago.\\n\\n## Summary\\n\\nThose of us in the WebAssembly and, more specifically, _wasmCloud_, community are not only looking forward to this bright future, but doing everything in our power to make it happen. If you\'re interested in helping us usher in this new era, please join our [slack](https://slack.wasmcloud.com) or our weekly community meetings (1PM EST on Wednesdays. Stop into our slack for the Zoom invite) or look for places to contribute to our code.\\n\\n[^1]: Most organizations who think this will work out of the box without lots of re-engineering will be disappointed.\\n[^2]: Note that it will take a lot of effort, collaboration, standards work, and coding to bring about this utopian future. But it _is_ possible."},{"id":"/wasmcloud_third_anniversary","metadata":{"permalink":"/blog/wasmcloud_third_anniversary","source":"@site/blog/wasmcloud_third_anniversary.mdx","title":"Reflections on Three Years of wasmCloud","description":"Reflections on Three Years of wasmCloud","date":"2022-06-25T15:00:00.000Z","formattedDate":"June 25, 2022","tags":[],"readingTime":6.565,"hasTruncateMarker":true,"authors":[{"name":"Kevin Hoffman"}],"frontMatter":{"title":"Reflections on Three Years of wasmCloud","image":"/img/waxosuit.png","date":"2022-06-25T15:00:00.000Z","author":"Kevin Hoffman","author_profile":"https://twitter.com/KevinHoffman","description":"Reflections on Three Years of wasmCloud","categories":["wasm","webassembly"],"draft":false},"prevItem":{"title":"WebAssembly and the Road to Ubiquity","permalink":"/blog/road_to_ubiquity"},"nextItem":{"title":"WebAssembly Components and wasmCloud Actors: A Glimpse of the Future","permalink":"/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future"}},"content":"![waxosuit](/img/waxosuit.png)\\n\\nIt\'s hard to believe that wasmCloud has been around for 3 years, and the inspiration and desire even longer.\\n\\nIt was a pleasant, cold winter\'s day in February of 2016. A friend of mine had sent me an inconspicuous link to an unassuming article. Surely this link would never send me down an inescapable rabbit hole from which I have yet to escape to this day. _Surely_.\\n\\n\x3c!--truncate--\x3e\\n\\nThe offending article was about this thing called the [Cloud ABI](https://lwn.net/Articles/674770/). The short version of the story behind Cloud ABI is that people were frustrated with the lack of security around the native plugin model. They were also frustrated with the lack of performance of secure isolation mechanisms like bulky virtual machines that provided better security than native plugins. Further, the folks behind this movement were also disappointed with the way in which capabilities could be granted to application code. Cloud ABI was meant to be the solution to these things and more.\\n\\nFast forward a year and, at least as far as I could tell, the efforts around Cloud ABI had stagnated, making way for this new thing called WebAssembly. WebAssembly\'s 1.0 specification aimed to deal with the security, isolation, performance, and portability problems that Cloud ABI wanted to tackle. Eventually, WASI would be born of the evolution of the Cloud ABI ideas around granting POSIX-like capabilities to user code. I don\'t mean to imply that there is direct lineage from Cloud ABI to WebAssembly, but you can see relationship and shared inspiration.\\n\\nI\'d been inspired about the concepts from Cloud ABI since early 2016. I\'d continued to follow the evolution of this idea through 2017 (WebAssembly was announced in 2015 and released in March of 2017) and into early 2018, where I dove head first into WebAssembly until I couldn\'t stand idly by any longer. I decided to write a book about it. I started writing [_Programming WebAssembly with Rust_](https://pragprog.com/titles/khrust/programming-webassembly-with-rust/) in an official capacity in June of 2018.\\n\\nIn the process of writing the book, in telling the \\"hero\'s journey\\" from beginner to expert in the WebAssembly ecosystem, I\'d developed an itch that needed scratching. The book would come out in March of 2019, but it wasn\'t enough for me. I knew what WebAssembly would ultimately be capable of, and I had a choice. I could sit back and wait for the ecosystem to catch up to Wasm\'s potential, or I could roll up my sleeves and start building it myself. For those doing the math, my book came out just 3 months after WebAssembly became a W3C recommendation.\\n\\nWhen I started building, the first question I asked, before anything else, was, _\\"Can I embed security information into a WebAssembly module so that it doesn\'t require consulting a central authority to validate?\\"_. This was before _\\"will it blend?\\"_ and long before I ever figured out what would come to be called the [lattice](https://wasmcloud.dev/reference/lattice/). As a proof of concept, I found that I could place signed JSON Web Tokens (JWT) into a Wasm module without interfering with its execution. I could sign a module with a list of _capability claims;_ a finite list of what this module was allowed to access. This stepping-off point ultimately led to the GitHub release of the first prototype of waxosuit in June of 2019, so named because we likened the framework to an _exosuit_ into which you could place your WebAssembly modules. We subsequently renamed it to [wasmCloud](https://github.com/wasmCloud), but we will always have a place in our heart for the waxosuit logo (the header image for this blog).\\n\\nI guess this is the part where I get to say that, _back in my day, our Wasm runtimes\\nhad to walk uphill both ways in the snow, and we liked it_. The first version of\\n`waxosuit` was a standalone Rust binary (about 23MB on Linux) that could read a manifest\\nfile and use it to start actors and providers and configure them. It had no networking\\nsupport outside of the providers--hosts were an island unto themselves.\\n\\nThe first version of the runtime was built with Rust\'s \\"heavy\\" threads and communication between them was done via channels. There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became too much spaghetti to maintain, and we refactored.\\n\\nThis refactor switched from \\"heavy\\" threads to using an actor-model crate for Rust called\xa0[Actix](https://actix.rs/book/actix/sec-2-actor.html). It made a lot of sense at the time--We were building an actor-based host runtime so an actor model for the internal plumbing felt harmonious. It did clean up the spaghetti from the heavy threads and we were able to see some performance improvements in function calls. However, Actix is based on Rust\'s async model, and carries all of the baggage that entails. The more we added features to this new runtime, the more difficult it became to develop. Ultimately, the only people who could touch certain parts of the codebase were those who knew the precise magical incantation of syntax to get the futures+Actix code to work.\\n\\nThis violated one of the core philosophies of wasmCloud. Every aspect of the codebase needed to be accessible to everyone. This new code was inscrutable even to those who wrote it. The key smell occurred when we were planning upcoming features and people visibly recoiled when we suggested large enhancements to the core Actix (async) plumbing.\\n\\nIt felt like we were rewriting large portions of Erlang\'s OTP in Rust. In fact, it felt so much like it that we made a huge decision to rewrite the host runtime in Elixir. We looked at where we were spending our time and it turned out to be the very same horrible 90/10 split that we\'re trying to eliminate with wasmCloud. We spent 90% of our time working on things that weren\'t core features like async and concurrency and thread-safe queue and dispatch management, and 10% actual features. The proposition was that if we \\"simply\\" built on top of OTP, we could stop rewriting OTP in Rust and instead focus on wasmCloud-specific features.\\n\\nThis was a\xa0*huge*\xa0undertaking. There is fantastic support for calling Rust functions from Elixir, so we knew we wouldn\'t need to completely rewrite _all_ of our crates. We thought it was going to be an enormous task and, of course, it turned out to be twice as long and difficult as originally expected. Even with all of the effort and time we put into it, we still knew it had been worth it. Immediately our runtime was able to do more things with better and more clear concurrency.\\n\\nThe codebase got less intimidating, but it was still daunting because Elixir is new to a lot of people. However, we\'d rather walk people through a functional programming forest than have them hack their way through a code jungle armed with nothing but a fork.\\n\\nIt grew easier to add new features and functionality to the host that were tolerant to crashes and ran concurrently while still consuming a small amount of resources. We learned a\xa0*ton*\xa0about running Elixir in production environments, but that\'s likely a topic for a whole series of blog posts on their own.\\n\\nIt\'s hard to imagine that it\'s been three years. Since we started, the community has grown, we\'ve steadily increased the number of people who attend our weekly community calls, the slack regularly gets new members, and we routinely see contributions from new folks and are continually surprised by what people are building using wasmCloud.\\n\\nWith all this in our rear view mirror, the best is yet to come. With all that we\'ve experienced and learned, we can clearly see what needs to be done to continually improve the developer experience so that wasmCloud will be\xa0*the*\xa0way to build secure, portable, distributed applications that can dynamically scale from monoliths to globally redundant services with the flip of a switch."},{"id":"/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future","metadata":{"permalink":"/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future","source":"@site/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future.md","title":"WebAssembly Components and wasmCloud Actors: A Glimpse of the Future","description":"Using the Component Model with wasmCloud Actors","date":"2022-06-16T15:00:00.000Z","formattedDate":"June 16, 2022","tags":[],"readingTime":15.665,"hasTruncateMarker":true,"authors":[{"name":"Taylor Thomas"}],"frontMatter":{"title":"WebAssembly Components and wasmCloud Actors: A Glimpse of the Future","image":"/img/wasm.png","date":"2022-06-16T15:00:00.000Z","author":"Taylor Thomas","author_profile":"https://twitter.com/_oftaylor","description":"Using the Component Model with wasmCloud Actors","categories":["wasm","webassembly","components"],"draft":false},"prevItem":{"title":"Reflections on Three Years of wasmCloud","permalink":"/blog/wasmcloud_third_anniversary"},"nextItem":{"title":"Building Portable, Scalable Components with TinyGo and wasmCloud","permalink":"/blog/example_creating_webassembly_actor_in_go_with_tinygo"}},"content":"![wasm](/img/wasm.png)\\n\\nToday we thought we would give you a glimpse of the future of WebAssembly and wasmCloud. As\\nwasmCloud maintainers, we\'ve always had a goal to follow all standards in the WebAssembly community.\\nHowever, our other goal has been to create a platform on which you could leverage the power of Wasm\\nfor real projects. For the last few years, these two goals have been somewhat at odds with one\\nanother due to the bleeding-edge nature of Wasm. We\'ve had to bridge the gap between Wasm\'s current\\nstate and the requirements needed to do Something Real\u2122 with it. This is starting to change!\\n\\n\x3c!--truncate--\x3e\\n\\n## The Component Model\\n\\nIn the past year, the standards around Wasm and WASI have started to solidify and become reality.\\nOne of the most interesting emerging standards has been the [Component\\nModel](https://github.com/WebAssembly/component-model). The TL;DR of the component model is that you\\nare able to glue together arbitrary Wasm modules that import or export functions, as specified by an\\ninterface file. These interface files are called `wit` files (Wasm Interface Types) and allow for\\nlanguage agnostic code generation. This code is what handles converting the raw numbers of plain\\nWasm (i.e. integers and bytes) into concrete types. If you are familiar with wasmCloud already, this\\nis [very similar](https://wasmcloud.dev/interfaces/) to what we call \\"contract driven development,\\"\\nwhich we use to separate non-functional requirements from business logic. Still confused? Don\'t\\nworry, we\'ll be using some specific examples below. If this topic interests you and you\'d like more\\ninformation, we highly recommend you check out all of the\\n[documentation](https://github.com/WebAssembly/component-model/tree/main/design) and\\n[examples](https://radu-matei.com/blog/intro-wasm-components/) that are available for the component\\nmodel.\\n\\n## A New Way to Build Actors\\n\\nSo at this point, you are probably wondering \\"What does this have to do with wasmCloud? Don\'t you\\nalready have your own contract stuff and RPC protocol?\\" Good question! Let\'s dive into this.\\n\\nWe recently created a proof of concept that shows our [kvcounter\\nexample](https://github.com/wasmCloud/examples/blob/557770a1d1d763aab76583af9f57e2a4e2aa4e3a/actor/kvcounter/README.md)\\nusing the Component Model to provide all the necessary logic that used to be provided by our\\n`wasmbus-rpc` Rust crate and other language specific libraries. Please note that this is not a fully\\nfunctional example that can run in wasmCloud currently; it\'s meant to show how we can glue together\\nvarious components, call the actor from the host, and then have the actor send data back to the\\nhost. We\'ll break down all the different parts of this example below, but you can find the actual\\nsource code [here](https://github.com/wasmCloud/examples/tree/spike/wit-interfaces/actor/kvcounter).\\n\\n### Old vs new\\n\\n![components-diagram](/img/components_diagram.png)\\n\\nAs you can see in the diagram above, there was a lot of stuff that had to be done inside of\\nlanguage-specific and user-managed code. In fact, we just finished writing all of this code for Go\\nas well, and it was a heavy lift! In the future version, things become significantly more modular.\\nThe communication to the host (which then gets sent over our RPC layer, [the\\nlattice](https://wasmcloud.dev/reference/lattice/)) is handled by one module, and then we just\\nprovide a single Wasm module that satisfies each interface. This highlights why we are excited about\\nWasm. Wasm is language agnostic, which means that instead of having to create special `wasmbus-rpc`\\nlibraries for every single language we want to support, and then generating code for every\\ninterface, we can now write a single module in whatever language we want. Then, we can use that\\nmodule to provide the necessary code to a wasmCloud actor written _in any other language_. If that\\ndoesn\'t make you excited, we don\'t know what will.\\n\\n### The detailed view\\n\\nFrom here on out, we are going to go into the specifics of how everything works. This will likely be\\nuseful to anyone wanting to experiment with the component model or who wants to better understand\\nhow it works. If that doesn\'t interest you, please feel free to skip down to the [\\"What did we\\nlearn?\\" section](#what-did-we-learn)\\n\\nWe\'re going to go from the top to the bottom as represented in the diagram above to explain what\\neach component does, so please go back and reference it if anything below is confusing. Then we\'ll\\nexplain how we linked and ran the example.\\n\\n#### The httpserver \\"receiver\\"\\n\\nUnder the new paradigm, each wasmCloud interface will need to have two different WebAssembly\\nmodules: a sender and a receiver. The receiver half is used when the actor needs to receive a\\nmessage from the host it is running on (hence the name). In wasmCloud, this means the host will\\nreceive a message on [the lattice](https://wasmcloud.dev/reference/lattice/) and then invoke the\\n`receive` method of the actor. This first module of the proof of concept is the httpserver contract\\nreceiver. In order to implement this, the module needs to import one interface and export another.\\nHere are what the interfaces look like:\\n\\n**wasmbus_receiver.wit**\\n\\n```plain\\n// These are importing some common types that you see in the receive function signature. See the\\n// actual code on Github if you are curious what these types look like\\nuse * from error-type\\nuse * from wasmbus-common\\n\\nreceive: function(msg: message) -> expected<payload, rpc-error>\\n```\\n\\nYou\'ll see that this interface has single function, the `receive` function that allows an actor to\\nreceive a message. We\'ll see how this works in code below\\n\\n**httpserver.wit**\\n\\n```plain\\nuse * from error-type\\n\\ntype header-map = list<tuple<string, string>>\\n\\nrecord http-request {\\n    // HTTP method. One of: GET,POST,PUT,DELETE,HEAD,OPTIONS,CONNECT,PATCH,TRACE\\n    method: string,\\n    // full request path\\n    path: string,\\n    // query string. May be an empty string if there were no query parameters.\\n    query-string: string,\\n    // map of request headers (string key, string value)\\n    header: header-map,\\n    body: list<u8>,\\n}\\n\\nrecord http-response {\\n    // statusCode is a three-digit number, usually in the range 100-599 a value of 200 indicates success.\\n    status-code: u16,\\n    // Map of headers (string keys, list of values)\\n    header: header-map,\\n    // Body of response as a byte array. May be an empty array.\\n    body: list<u8>,\\n}\\n\\nhandle-request: function(req: http-request) -> expected<http-response, rpc-error>\\n```\\n\\nThe httpserver receiver has to export the `receive` method and implement the logic to parse that\\nmessage as an HTTP request. Basically, it acts as a translation layer between the RPC layer and the\\nactual contract it needs to call. The code for it is actually quite straightforward (and is\\nannotated with comments below). Our code was written in Rust, but you could write it in any language\\nthat has [wit-bindgen](https://github.com/bytecodealliance/wit-bindgen/) support\\n\\n```rust\\nuse wasmbus_receiver::*;\\n\\n// Import the httpserver contract so we can call it\\nwit_bindgen_rust::import!(\\"../httpserver.wit\\");\\n// Export our implementation of the `receive` method\\nwit_bindgen_rust::export!(\\"../wasmbus-receiver.wit\\");\\n\\nconst HANDLE_REQUEST_METHOD: &str = \\"HttpServer.HandleRequest\\";\\n\\n// Some custom types for massaging data are elided here\\n\\n#[derive(Default, Clone)]\\npub struct WasmbusReceiver;\\n\\nimpl wasmbus_receiver::WasmbusReceiver for WasmbusReceiver {\\n    fn receive(msg: Message) -> Result<Payload, RpcError> {\\n        if msg.method != HANDLE_REQUEST_METHOD {\\n            return Err(RpcError::MethodNotHandled(format!(\\n                \\"Method {} is not supported by the httpserver contract\\",\\n                msg.method\\n            )));\\n        }\\n        // Parse the message body into an http request\\n        let req: HttpRequestInternal = serde_json::from_slice(&msg.arg)\\n            .map_err(|e| RpcError::Deser(format!(\\"httpserver: {}\\", e)))?;\\n        // Data massaging\\n        let header: Vec<(&str, &str)> = req\\n            .header\\n            .iter()\\n            .map(|(k, v)| (k.as_str(), v.as_str()))\\n            .collect();\\n\\n        // Call the `handle_request` method that will be provided by another module\\n        let resp: HttpResponseInternal = httpserver::handle_request(httpserver::HttpRequest {\\n            method: &req.method,\\n            path: &req.path,\\n            query_string: &req.query_string,\\n            header: &header,\\n            body: &req.body,\\n        })\\n        .map_err(httpserver_to_wasmbus_error)?\\n        .into();\\n        serde_json::to_vec(&resp).map_err(|e| RpcError::Ser(e.to_string()))\\n    }\\n}\\n```\\n\\n#### The Business Logic\\n\\nThis module is the actual part that contains the business logic that someone would be writing. This\\nis the only user-provided code for someone writing their business logic to run on wasmCloud. All of\\nthe other modules described in this section would be provided by the interface writer or by\\nwasmCloud directly.\\n\\nThis code also requires the use of two interfaces: httpserver (see the previous section) and\\nkeyvalue:\\n\\n**keyvalue.wit**\\n\\n```plain\\nuse * from error-type\\n\\n// Increment the value of the key by the given amount\\nincrement: function(key: string, value: s32) -> expected<s32, rpc-error>\\n```\\n\\nPlease note that this is a stripped-down version of what the actual keyvalue contract would look\\nlike, to keep things simple. Now, we can move on to the actual code (annotated):\\n\\n```rust\\nuse httpserver::*;\\n\\n// Import the keyvalue contract so we can call it\\nwit_bindgen_rust::import!(\\"../keyvalue.wit\\");\\n\\n// Export our implementation of the httpserver contract\\nwit_bindgen_rust::export!(\\"../httpserver.wit\\");\\n\\n#[derive(Default, Clone)]\\npub struct Httpserver;\\n\\nimpl httpserver::Httpserver for Httpserver {\\n    fn handle_request(req: HttpRequest) -> Result<HttpResponse, RpcError> {\\n        // make friendlier key\\n        let key = format!(\\"counter:{}\\", req.path.replace(\'/\', \\":\\"));\\n\\n        // bonus: use specified amount from query, or 1\\n        let amount: i32 = form_urlencoded::parse(req.query_string.as_bytes())\\n            .find(|(n, _)| n == \\"amount\\")\\n            .map(|(_, v)| v.parse::<i32>())\\n            .unwrap_or(Ok(1))\\n            .unwrap_or(1);\\n\\n        // increment the value in kv and send response in json\\n        let (body, status_code) = match increment_counter(key, amount) {\\n            Ok(v) => (serde_json::json!({ \\"counter\\": v }).to_string(), 200),\\n            // if we caught an error, return it to client\\n            Err(e) => (\\n                serde_json::json!({ \\"error\\": format!(\\"{:?}\\", e) }).to_string(),\\n                500,\\n            ),\\n        };\\n        let resp = HttpResponse {\\n            body: body.as_bytes().to_vec(),\\n            status_code,\\n            header: Vec::new(),\\n        };\\n        Ok(resp)\\n    }\\n}\\n\\nfn increment_counter(key: String, value: i32) -> Result<i32, RpcError> {\\n    // Call the `increment` function that will be provided by another module\\n    keyvalue::increment(&key, value).map_err(map_wit_err)\\n}\\n```\\n\\nYou\'ll see that this code looks almost identical to the [original kvcounter\\nactor](https://github.com/wasmCloud/examples/blob/557770a1d1d763aab76583af9f57e2a4e2aa4e3a/actor/kvcounter/src/lib.rs#L1),\\nexcept that there is **_zero wasmCloud-specific code needed_**!\\n\\n#### The keyvalue \\"sender\\"\\n\\nAs we mentioned above, there are two halves needed for each contract. This component exports the\\nkeyvalue contract described above and also requires one other interface:\\n\\n**wasmbus-sender.wit**\\n\\n```plain\\nuse * from error-type\\nuse * from wasmbus-common\\n\\nsend: function(msg: message, contract-name: string, link-name: option<string>) -> expected<payload, rpc-error>\\n```\\n\\nAs you can see, this has a single function called `send` that is used to send a message through the\\nlattice. This does the exact reverse of the receiver, in that it takes a concrete type and turns it\\ninto a generic message that can be sent. The annotated code is below:\\n\\n```rust\\nuse keyvalue::*;\\nuse wasmbus_sender as wasmbus;\\n\\n// Export our implementation of the keyvalue contract\\nwit_bindgen_rust::export!(\\"../keyvalue.wit\\");\\n// Import the sender contract for us to call\\nwit_bindgen_rust::import!(\\"../wasmbus-sender.wit\\");\\n\\n// Custom request type elided\\n\\n#[derive(Default, Clone)]\\npub struct Keyvalue;\\n\\nimpl keyvalue::Keyvalue for Keyvalue {\\n    fn increment(key: String, value: i32) -> Result<i32, RpcError> {\\n        // Encode the data as our payload\\n        let payload = serde_json::to_vec(&IncrementRequest { key, value })\\n            .map_err(|e| RpcError::Ser(e.to_string()))?;\\n\\n        // Call the `send` method provided by another module\\n        // NOTE: this code is not dealing with the link name yet just to keep it simple\\n        // We will figure out how we want this to work when implementing\\n        let resp = wasmbus::send(\\n            wasmbus::Message {\\n                method: \\"KeyValue.Increment\\",\\n                arg: &payload,\\n            },\\n            \\"wasmcloud:keyvalue\\",\\n            None,\\n        )\\n        .map_err(wasmbus_to_keyvalue_error)?;\\n        serde_json::from_slice(&resp).map_err(|e| RpcError::Deser(e.to_string()))\\n    }\\n}\\n```\\n\\n#### The host sender\\n\\nLast, but not least, is the module that can send a message back to a host (so the host can send it\\non the lattice). For our purposes here, this just prints to stdout (which means we are using a\\nfunction from the host just like we would for real), but when we do it for realsies, this will be\\ncalling a specific function the host will provide for us. This module only requires the\\n`wasmbus-sender.wit` contract shown in the previous section. As for the code:\\n\\n```rust\\nuse wasmbus_sender::*;\\n\\n// Export our implementation for the `send` method\\nwit_bindgen_rust::export!(\\"../wasmbus-sender.wit\\");\\n\\n#[derive(Default, Clone)]\\npub struct WasmbusSender;\\n\\nimpl wasmbus_sender::WasmbusSender for WasmbusSender {\\n    fn send(\\n        msg: Message,\\n        contract_name: String,\\n        link_name: Option<String>,\\n    ) -> Result<Payload, RpcError> {\\n        // Fake a host call (fd_write in this case)\\n        println!(\\n            \\"Linkname: {}, contract_name: {}, msg: {:#?}\\",\\n            link_name.unwrap_or_else(|| \\"default\\".to_string()),\\n            contract_name,\\n            msg\\n        );\\n        // Return the answer to everything\\n        Ok(serde_json::to_vec(&42).unwrap())\\n    }\\n}\\n```\\n\\n#### Linking and running\\n\\n:::caution\\n**NOTE**: Right as we were preparing this blog post, the `wasmlink` command and tooling\\nwas [removed](https://github.com/bytecodealliance/wit-bindgen/pull/240) from the wit-bindgen repo in\\nfavor of the [most up to date component model\\ncode](https://github.com/bytecodealliance/wit-bindgen/pull/239). This new component model tooling is\\ngoing to be the future, but currently, there isn\'t really a replacement for `wasmlink`. So the\\nsection below is slightly out of date, but still shows that all of this will work with the new\\ntooling in the future. As we actually implement this, we will release a new blog post that shows how\\nthe new tooling works\\n:::\\n\\nFor our proof of concept, we used the `wasmlink` command. When we do this for real, we will use the\\nunderlying Rust linker library that `wasmlink` uses. To be honest, this tool is a little confusing\\nto use, so hopefully we can enlighten you here. Before linking, we built all of the modules in the\\nworkspace by running `cargo build --release`. Once they were built, we ran the following command to\\nlink them together\\n\\n```terminal\\nwasmlink ./target/wasm32-wasi/release/httpserver.wasm \\\\\\n   -m keyvalue=./target/wasm32-wasi/release/keyvalue.wasm \\\\\\n   -m httpserver=./target/wasm32-wasi/release/kvcounter_actor.wasm \\\\\\n   -m wasmbus-sender=./target/wasm32-wasi/release/wasmbus_sender.wasm \\\\\\n   -i wasmbus-sender=wasmbus-sender.wit \\\\\\n   -i keyvalue=keyvalue.wit \\\\\\n   -i httpserver=httpserver.wit \\\\\\n   -i receiver=wasmbus-receiver.wit \\\\\\n   -p wasmtime \\\\\\n   -o compiled.wasm\\n```\\n\\nOk, so that is a pretty gnarly command. Let\'s break it down:\\n\\nFirst off is the module name (`./target/wasm32-wasi/release/httpserver.wasm`). There is one very\\nimportant detail here. This module should be the one you want to call (the one with the `receive`\\nfunction), otherwise the export gets mangled and no longer shows up in the compiled file.\\n\\nAll of the `-m` flags specify the other modules to link in. They are specified in the form of\\n`MODULE_NAME=MODULE_PATH`. The module name must match the name of the interface it is exporting.\\nThose interfaces are specified with the `-i` flag with the form `MODULE_NAME=WIT_PATH`. The `-o`\\nflag specifies the output path where the compiled module is written to.\\n\\nOh, and that `-p` flag? Pretty sure it doesn\'t matter based on what we found in the code, but it is\\na required flag. It does look like it may matter in the future though.\\n\\nWhew...that was hard. Onward to the cool part \u2013 actually running the thing. We did this in code as\\nit was needed to actually call everything properly. Let\'s look at the whole code sample (annotated):\\n\\n```rust\\nuse wasmtime::{Config, Engine, Linker, Module, Store};\\nuse wasmtime_wasi::WasiCtx;\\n\\n// Import the wasmbus-receiver contract with the wasmtime helpers (note that this is a different\\n// crate than what we used above)\\nwit_bindgen_wasmtime::import!(\\n    \\"/Users/oftaylor/Documents/code/examples/actor/kvcounter/wasmbus-receiver.wit\\"\\n);\\n\\n// Elided an http request type here\\n\\n// A custom struct for storing data in the wasmtime engine\\nstruct StoreData {\\n    wasi: WasiCtx,\\n    receiver: wasmbus_receiver::WasmbusReceiverData,\\n}\\n\\nfn main() {\\n    let mut config = Config::default();\\n    // Enable the experimental module linking and multimemory proposals. These are required to make things work\\n    config.wasm_module_linking(true);\\n    config.wasm_multi_memory(true);\\n    let engine = Engine::new(&config).unwrap();\\n\\n    let mut linker: Linker<StoreData> = Linker::new(&engine);\\n    // Add all the wasi stuff to the linker\\n    wasmtime_wasi::add_to_linker(&mut linker, |ctx| &mut ctx.wasi)\\n        .expect(\\"Unable to add to wasi things to linker\\");\\n\\n    let wasi = wasmtime_wasi::WasiCtxBuilder::new()\\n        .inherit_stdio()\\n        .inherit_args()\\n        .expect(\\"Unable to inherit args\\")\\n        .build();\\n\\n    let mut store = Store::new(\\n        &engine,\\n        StoreData {\\n            wasi,\\n            receiver: wasmbus_receiver::WasmbusReceiverData {},\\n        },\\n    );\\n\\n    // Load the compiled wasm module we built above\\n    let receiver_module = Module::from_file(\\n        &engine,\\n        \\"/code/examples/actor/kvcounter/compiled.wasm\\",\\n    )\\n    .unwrap();\\n\\n    // Use the instantiate helper from wit-bindgen\\n    let (server, _instance) = wasmbus_receiver::WasmbusReceiver::instantiate(\\n        &mut store,\\n        &receiver_module,\\n        &mut linker,\\n        |ctx| &mut ctx.receiver,\\n    )\\n    .unwrap();\\n\\n    let req = HttpRequestInternal {\\n        method: \\"GET\\",\\n        path: \\"/\\",\\n        query_string: \\"\\",\\n        header: vec![(\\"HOST\\", \\"foobar\\")],\\n        body: &[],\\n    };\\n    // Call the receive method exported by our module\\n    let resp = server\\n        .receive(\\n            &mut store,\\n            wasmbus_receiver::Message {\\n                method: \\"HttpServer.HandleRequest\\",\\n                arg: &serde_json::to_vec(&req).expect(\\"Should serialize\\"),\\n            },\\n        )\\n        .expect(\\"Shouldn\'t get a trap\\")\\n        .expect(\\"Unable to send to actor\\");\\n\\n    println!(\\"body: {}\\", String::from_utf8_lossy(&resp));\\n}\\n```\\n\\nWhen we ran this, we could see the output:\\n\\n```\\nLinkname: default, contract_name: wasmcloud:keyvalue, msg: Message {\\n    method: \\"KeyValue.Increment\\",\\n    arg: [\\n        123,\\n        34,\\n        107,\\n        101,\\n        ...\\n    ],\\n}\\nbody: {\\"status_code\\":200,\\"header\\":[],\\"body\\":[123,34,99,111,117,110,116,101,114,34,58,52,50,125]}\\n```\\n\\nThis means we got all the way down to the `send` method and returned data all the way up the stack\\nas the expected HTTP response!\\n\\n## What did we learn?\\n\\n### Benefits\\n\\n- We won\'t need to use our bespoke Smithy + code generation any more\\n- No more bespoke wasmbus libraries per language. Modules can even be loaded by providers to\\n  properly translate a message from the lattice\\n- No wasmCloud-specific code when you write your actors. In fact, if our contract is the same as\\n  those used by other platforms, they could even be interchangeable!\\n- Easily pluggable and patchable wasmCloud specific code. If there is a bug fix we have to the\\n  underlying RPC protocol, we can hot patch all running actors with no user interaction\\n\\n### Rough edges\\n\\nTo be clear, it isn\'t all sunshine and rainbows yet. These are a few of the rough edges we\\nencountered and how they impacted us\\n\\n- No dynamic linking yet. This means we have to manually pull everything down and link it before\\n  we can run it. Not ideal, but we are able to do it through code.\\n- Linking everything means you must include both the module and the interface file when\\n  distributing things, which means you have to build tooling around building things like\\n  [bindles](https://github.com/deislabs/bindle)\\n- Even when you have reused types (like our `rpc-error` above in the wit files), each interface\\n  technically has a different type in strongly-typed languages. This requires conversion between\\n  the identical types imported from different interfaces. Obviously things like Rust macros can be\\n  use to make this a little less clunky, but it is a bit of a chore\\n\\n## Where do we go from here?\\n\\nNow we move into the future. To be absolutely clear, YOU CANNOT yet do this inside of wasmCloud, but\\nthis proof of concept proved that we can use the component model to greatly improve the experience\\nof writing actors in wasmCloud _and_ achieve our goal of being in line with community standards. In\\norder to make this all work, it will take a major refactor of the underlying code we use to run\\nactors as well as some refactors to our RPC layer. This will obviously be a breaking change so we\\nwill need to clearly communicate when the work is going to land so as to not disturb too many of our\\ncurrent users.\\n\\nWe will also need to rely more heavily on [Bindle](https://github.com/deislabs/bindle) and\\neventually on the forthcoming component registry work from the Bytecode Alliance. These tools are\\ndesigned specifically to account for assembling various parts of a final application (like the\\nvarious interfaces and different modules). We already have [experimental support for\\nbindles](https://wasmcloud.dev/reference/bindle/) in wasmCloud, but they have to be hand rolled\\nrather than being automatically created. There also needs to be a place from which you can fetch the\\nnecessary interfaces for use in building. All of these elements of developer experience are\\nimportant to have before we roll this out.\\n\\nSo, stay tuned! We are planning on a follow up blog post to this one once we actually roll out the\\nsupport in wasmCloud\\n\\n## Special Thanks\\n\\nWe wanted to give a shout out and thanks to [Radu Matei](https://radu-matei.com/) for his help as we\\nfigured out some of the intricacies of the component model, as well as his previous work and blog\\nposts in this area. That work gave us a great starting place for what we are building here."},{"id":"/example_creating_webassembly_actor_in_go_with_tinygo","metadata":{"permalink":"/blog/example_creating_webassembly_actor_in_go_with_tinygo","source":"@site/blog/example_creating_webassembly_actor_in_go_with_tinygo.md","title":"Building Portable, Scalable Components with TinyGo and wasmCloud","description":"A walkthrough of creating a TinyGo wasmCloud actor","date":"2022-06-01T13:00:00.000Z","formattedDate":"June 1, 2022","tags":[],"readingTime":4.145,"hasTruncateMarker":true,"authors":[{"name":"Kevin Hoffman"}],"frontMatter":{"title":"Building Portable, Scalable Components with TinyGo and wasmCloud","image":"/img/tinygo-logo.png","date":"2022-06-01T13:00:00.000Z","author":"Kevin Hoffman","author_profile":"https://www.linkedin.com/in/%F0%9F%A6%80-kevin-hoffman-9252669/","description":"A walkthrough of creating a TinyGo wasmCloud actor","categories":["tinygo","webassembly","wasmcloud","go","example"],"draft":false},"prevItem":{"title":"WebAssembly Components and wasmCloud Actors: A Glimpse of the Future","permalink":"/blog/webassembly_components_and_wasmcloud_actors_a_glimpse_of_the_future"},"nextItem":{"title":"wasmCloud Capabilities are Managed Algebraic Effects for WebAssembly Functions","permalink":"/blog/caps_are_effects"}},"content":"![tinygo-logo](/img/tinygo-logo.png)\\n\\n_[TinyGo](https://tinygo.org)_ is _\\"a Go compiler for small places\\"_. It is a language designed specifically to work on embedded systems and WebAssembly. If you squint hard enough, you can almost imagine that WebAssembly is a form of embedded system (it\'s embedded in a host runtime).\\n\\nOne of the core tenets of wasmCloud has always been that we embrace the specification without doing anything proprietary. In other words, anyone who knows the \\"<u>[wasmCloud ABI](https://wasmcloud.dev/reference/wasmbus/ffi/)</u>\\" can create actors in any language that compiles to freestanding WebAssembly. While this is technically true, it\'s certainly a lot easier when we have an easy SDK and code generation support for a language. Using our SDKs gives you a more friendly library while helping insulate your code from changes to the underlying WebAssembly spec.\\n\\nThe newest language in our arsenal is TinyGo.\\n\\n\x3c!--truncate--\x3e\\n\\nTo get started, you\'ll need <u>[wash](https://github.com/wasmcloud/wash)</u> version `0.11.0` or newer.\\n\\nLet\'s create a new empty actor from a template as follows:\\n\\n```terminal\\n $ wash new actor\\n? Select a project template: \u203a\\n  hello: a hello-world actor (in Rust) that responds over an http connection\\n\u276f echo-tinygo: a hello-world actor (in TinyGo) that responds over an http connection\\n```\\n\\nI\'m going to call this new project `kvcounter` because, for this blog post, we\'re going to build an actor that exposes a RESTful interface to a counter service.\\n\\nThis is the actor we get \\"out of the box\\":\\n\\n```go\\npackage main\\n\\nimport (\\n\\t\\"github.com/wasmcloud/actor-tinygo\\"\\n\\t\\"github.com/wasmcloud/interfaces/httpserver/tinygo\\"\\n)\\n\\nfunc main() {\\n\\tme := Kvcounter{}\\n\\tactor.RegisterHandlers(httpserver.HttpServerHandler(&me))\\n}\\n\\ntype Kvcounter struct{}\\n\\nfunc (e *Kvcounter) HandleRequest(\\n\\tctx *actor.Context,\\n\\treq httpserver.HttpRequest)\\n\\t(*httpserver.HttpResponse, error) {\\n\\tr := httpserver.HttpResponse{\\n\\t\\tStatusCode: 200,\\n\\t\\tHeader:     make(httpserver.HeaderMap, 0),\\n\\t\\tBody:       []byte(\\"hello\\"),\\n\\t}\\n\\n\\treturn &r, nil\\n}\\n```\\n\\nIn the preceding code, the `RegisterHandlers` function sets up the appropriate dispatch so that when the bound HTTP server capability provider receives a request, it knows to invoke this actor.\\n\\nWhat we\'re going to do for this blog post is modify this web request handler so that it takes the name of a counter from the request, increments it using the key-value interface, and returns the new value in response.\\n\\nFirst, let\'s add another provider interface to our imports by first running `go get`\\n\\n```terminal\\ngo get github.com/wasmcloud/interfaces/keyvalue/tinygo\\n```\\n\\nThis will modify our `go.mod` file to contain the new interface. Now let\'s create a new version of the `HandleRequest` function:\\n\\n```go\\nfunc (e *Kvcounter) HandleRequest(\\n\\tctx *actor.Context,\\n\\treq httpserver.HttpRequest) (*httpserver.HttpResponse, error) {\\n\\n\\tkey := strings.Replace(req.Path, \\"/\\", \\"_\\", -1)\\n\\n\\tkv := keyvalue.NewProviderKeyValue()\\n\\n\\tcount, err := kv.Increment(ctx, keyvalue.IncrementRequest{\\n\\t\\tKey: key, Value: 1,\\n\\t})\\n\\tif err != nil {\\n\\t\\treturn InternalServerError(err), nil\\n\\t}\\n\\n\\tres := \\"{\\\\\\"counter\\\\\\": \\" + strconv.Itoa(int(count)) + \\"}\\"\\n\\n\\tr := httpserver.HttpResponse{\\n\\t\\tStatusCode: 200,\\n\\t\\tHeader:     make(httpserver.HeaderMap, 0),\\n\\t\\tBody:       []byte(res),\\n\\t}\\n\\treturn &r, nil\\n}\\n\\nfunc InternalServerError(err error) *httpserver.HttpResponse {\\n\\treturn &httpserver.HttpResponse{\\n\\t\\tStatusCode: 500,\\n\\t\\tHeader:     make(httpserver.HeaderMap, 0),\\n\\t\\tBody:       []byte(err.Error()),\\n\\t}\\n}\\n```\\n\\nIn this new function, we are converting the `Path` from the request into a key that will then be used in an `Increment` operation on the key-value store.\\n\\nSomething might look a little \\"off\\" in the code, and that\'s this line:\\n\\n```go\\nres := \\"{\\\\\\"counter\\\\\\": \\\\\\"\\" + strconv.Itoa(int(count)) + \\"\\\\\\"}\\"\\n```\\n\\nThis is something that we have to watch out for in TinyGo. If we use the stock JSON encoding/marshaling package, then TinyGo will use the following WebAssembly imports (shown in `wat`):\\n\\n```\\n(import \\"env\\" \\"runtime.ticks\\" (func $runtime.ticks (type 2)))\\n(import \\"env\\" \\"syscall/js.valueGet\\" (func $syscall/js.valueGet (type 3)))\\n(import \\"env\\" \\"syscall/js.valuePrepareString\\" (func $syscall/js.valuePrepareString (type 4)))\\n(import \\"env\\" \\"syscall/js.valueLoadString\\" (func $syscall/js.valueLoadString (type 3)))\\n(import \\"env\\" \\"syscall/js.finalizeRef\\" (func $syscall/js.finalizeRef (type 5)))\\n```\\n\\nTo get the preceding output, I typically run the following command (though use could also use `wasm-objdump`, too):\\n\\n```terminal\\nwasm2wat build/kvcounter_s.wasm| grep import\\n```\\n\\nThe `wasm2wat` binary is included in the <u>[wabt](https://github.com/WebAssembly/wabt)</u> toolkit.\\n\\nThere are still quite a few places in TinyGo where importing a certain package will trigger the use of the `syscall/js` package. Once this package is imported, the host runtime will then _require_ the use of these JavaScript host shims and we then immediately lose all of our portability benefits.\\n\\nTinyGo is rapidly plugging these holes and providing packages that don\'t require a JavaScript host runtime, but we still need to watch out for things like this. To keep this example simple rather than hunting for an alternative JSON encoder, we just created a string that contains valid JSON.\\n\\nNow, just like any other wasmCloud actor, we can modify the `CLAIMS` variable in the actor\'s `Makefile` to contain both the HTTP server contract and the Key-Value contract:\\n\\n```\\nCLAIMS   = --http_server --keyvalue\\n```\\n\\nWith our new TinyGo actor in hand, we can start the actor, start two capability providers (HTTP and Key-Value), provide a link definition, and finally curl the running endpoint:\\n\\n```\\n$ curl http://localhost:8080/bloggo\\n{\\"counter\\": 1}\\n$ curl http://localhost:8080/bloggo\\n{\\"counter\\": 2}\\n```\\n\\nThis is just the beginning of a really fun journey supporting TinyGo actors in wasmCloud!\\n\\nFor a fully functioning version of this sample, you can take a look at it in our <u>[examples repository](https://github.com/wasmCloud/examples/tree/main/actor/kvcounter-tinygo)</u>."},{"id":"/caps_are_effects","metadata":{"permalink":"/blog/caps_are_effects","source":"@site/blog/caps_are_effects.md","title":"wasmCloud Capabilities are Managed Algebraic Effects for WebAssembly Functions","description":"wasmCloud Capabilities are a managed, distributed implementation of algebraic effects","date":"2022-05-25T13:00:00.000Z","formattedDate":"May 25, 2022","tags":[],"readingTime":4.2,"hasTruncateMarker":true,"authors":[{"name":"Kevin Hoffman"}],"frontMatter":{"title":"wasmCloud Capabilities are Managed Algebraic Effects for WebAssembly Functions","image":"/img/algebra.jpg","date":"2022-05-25T13:00:00.000Z","author":"Kevin Hoffman","author_profile":"https://www.linkedin.com/in/%F0%9F%A6%80-kevin-hoffman-9252669/","description":"wasmCloud Capabilities are a managed, distributed implementation of algebraic effects","categories":["webassembly","wasmcloud"],"draft":false},"prevItem":{"title":"Building Portable, Scalable Components with TinyGo and wasmCloud","permalink":"/blog/example_creating_webassembly_actor_in_go_with_tinygo"},"nextItem":{"title":"Deploying wasmCloud Actors from Github Packages","permalink":"/blog/2022-05-23_ghcr-actions"}},"content":"![algebra](/img/algebra.jpg)\\n\\nWe spend a lot of time talking about how the wasmCloud capability provider system, from its abstract contracts to the ability to hot-swap providers, is a way to separate non-functional requirements from business logic code. While all of that is true, it\'s also a fairly _enterprisey_ way to describe it. In this blog post, I\'ll describe them another way using terms from functional programming.\\n\\n\x3c!--truncate--\x3e\\n\\nLet\'s take a look at a simple function:\\n\\n```\\nf(x) = x + 12\\n```\\n\\nThis function is _\\"pure\\"_. For every input of `x` there is only one output. In math terms that makes it a proper function, but in programming terms that means it\'s deterministic, and determinism means we can write unit tests for it. We could write a test that ensures that when I supply a `2` to this function, it returns `14`.\\n\\nUnfortunately, the second we expand our code beyond the realm of \\"hello world\\", it becomes more and more difficult to maintain purity. Our code needs to interact with the world, and it often does so in messy, unpredictable ways. Even if it isn\'t messy, we frequently see functions where the answer is only predictable for a short period of time.\\n\\nLet\'s take a look at this (psuedocode) function from a hypothetical bank back-end that handles an international withdrawal from a customer\'s account:\\n\\n```\\ninternationalWithdrawal account amount localCurrency =\\n    exchangeRate = Market.getRate(localCurrency)\\n    newAmount = amount * exchangeRate\\n    fee = Market.getFee(localCurrency)\\n    Ledger.withdraw(account, amount, newAmount, exchangeRate)\\n    Ledger.fee(account, fee)\\n    Ledger.balance(account)\\n```\\n\\nThis function reaches out to \\"the market\\" to get the currency exchange rate between the canonical currency and the local currency. Then it reaches out to get the current fee for international withdrawals. Finally, it consumes a ledger to perform the withdrawal, take away the international transaction fee, and finally return the current/updated balance.\\n\\nThere\'s nothing pure about this function, but it\'s an _extremely_ common idiom. This function relies on two external interactions, a `Market` and a `Ledger`. The exchange rate is something that fluctuates constantly throughout the day, and the ledger presumably gives the function access to the account ledger for a specific account.\\n\\nThis function is non-deterministic because it has a number of _algebraic effects_. Such effects are a category for the messy, \\"impure\\" things that happen in our functions. For most of us, we probably write more impure functions than not.\\n\\nI\'ve already tried to make this function fairly clean and simple. In contrast, I\'ve seen functions like this turned into \\"kitchen sinks\\" where a single function establishes a connection to two different databases and issues queries directly to them. We fool ourselves into thinking the function is pure by hiding the tight coupling a few layers down in the library, but this is still tight coupling at its worst. In cases like this, you can\'t test this function without live access to real databases, and after that you have to figure out how to make your tests deterministic (which often involves \\"test databases\\").\\n\\nA lot of us are used to patterns like (micro)services, abstractions, anti-corruption layers, and more all designed to help us mitigate the ugly side effects this function has. But what if we could embrace these effects and write functions that are explicit about their effect needs, _without_ losing testability, flexibility, and purity?\\n\\nIn a traditional object-oriented language or framework, we might treat each of these \\"effect providers\\" as an interface and then use something like dependency injection to shunt in an implementation for the effect at runtime (and presumably shunt in a mock during test time).\\n\\nIn wasmCloud, we manage algebraic effects through <u>[capability providers](https://wasmcloud.dev/reference/host-runtime/capabilities/)</u>. Here the capability provider, as seen by the WebAssembly module (<u>[actor](https://wasmcloud.dev/reference/host-runtime/actors/)</u>), is just an abstraction. It\'s a versioned contract through which the WebAssembly function gets its effects.\\n\\nThe host runtime is responsible for providing an implementation for those effects or effect providers. This implementation is hot-swappable and dynamically configurable. This means that in our preceding international withdrawal example, we could provide a \\"test market\\" at unit test time and then a real connection to the market service when running in production. We could also configure the market connection so it could be \\"real\\", but point to a different service in staging than in production.\\n\\nAlgebraic effects don\'t need to be big, high-level concepts like database or networking clients. Even something as basic as logging is an effect (because all I/O is \\"effectful\\"). So we might use yet another provider like this:\\n\\n```elixir\\nLogger.debug(\\"Performing international withdrawal\\")\\n```\\n\\nwasmCloud takes these algebraic effects even further by requiring each of our WebAssembly modules to be <u>[cryptographically signed](https://wasmcloud.dev/reference/host-runtime/security/)</u> with the explicit list of capabilities it can use (effects it can produce).\\n\\nUltimately what we\'ve done is provided a means to maintain portable function purity in WebAssembly modules while allowing for all algebraic effects to not only be testable, but distributed, hot-swappable, and dynamically scalable across a flat topology system comprised of multiple disparate environments.\\n\\nIf you\'re interested in learning more about capabilities and seeing them in action, take a look at our <u>[examples](https://github.com/wasmcloud/examples/)</u> repository."},{"id":"2022-05-23_ghcr-actions","metadata":{"permalink":"/blog/2022-05-23_ghcr-actions","source":"@site/blog/2022-05-23_ghcr-actions.md","title":"Deploying wasmCloud Actors from Github Packages","description":"Simplifying the deployment experience for WebAssembly modules.","date":"2022-05-23T13:00:00.000Z","formattedDate":"May 23, 2022","tags":[],"readingTime":5.86,"hasTruncateMarker":true,"authors":[{"name":"Brooks Townsend"}],"frontMatter":{"slug":"2022-05-23_ghcr-actions","title":"Deploying wasmCloud Actors from Github Packages","image":"/img/ghcr-actions/github-packages.png","date":"2022-05-23T13:00:00.000Z","author":"Brooks Townsend","author_profile":"https://linkedin.com/in/brooks-townsend","description":"Simplifying the deployment experience for WebAssembly modules.","categories":["webassembly","wasmcloud","developer experience"],"draft":false},"prevItem":{"title":"wasmCloud Capabilities are Managed Algebraic Effects for WebAssembly Functions","permalink":"/blog/caps_are_effects"}},"content":"![github-packages-logo](/img/ghcr-actions/github-packages.png)\\n\\nWith the general availability of GitHub Packages container registry, or GHCR for short, an easily accessible Docker registry made its way into the same platform many developers use for version control today. This was great news for containers and simplifying infrastructure, just like Actions greatly simplified workflows on GitHub.\\n\\nBut wait, there\'s more!\\n\\n\x3c!--truncate--\x3e\\n\\nGitHub container registry also supports the Open Container Initiative (OCI) specification, which doesn\'t limit \\"containers\\" to just Docker containers, and it supports public anonymous downloads! This makes GHCR a perfect target for hosting OCI-compliant artifacts, like wasmCloud actors and capability providers. Today, we\'re going to walk through evolving use cases for getting more power out of this feature of GitHub and how it can simplify your wasmCloud development beyond running on your local machine.\\n\\n## Prerequisites\\n\\nToday we\'ll be using a couple of tools for this tutorial:\\n\\n- A <u>[Rust](https://www.rust-lang.org/tools/install)</u> toolchain to build actors\\n  - Make sure to add wasm32 as a target with `rustup target add wasm32-unknown-unknown`\\n- .<u>[wash](https://wasmcloud.dev/overview/installation/#install-wash)</u>, the wasmCloud shell, at least `v0.11.0`\\n\\n## Pushing an actor to GitHub packages\\n\\nTo start, let\'s go ahead and generate a new wasmCloud actor project from the `hello` project template. This is our \\"hello world\\" actor.\\n\\n```bash\\nwash new actor hello\\n```\\n\\nOnce the project is generated, `cd hello` into the project. There, you can run `cargo build --release` build your actor module. The last step before we can push it to GitHub is to <u>[sign](https://wasmcloud.dev/app-dev/std-caps/#sign-the-actor)</u> the actor with embedded claims. The following command will sign your actor and allow it to access the HTTPServer capability:\\n\\n```bash\\nwash claims sign target/wasm32-unknown-unknown/release/hello.wasm --http_server --name Hello\\n```\\n\\nBy default this will place the actor under the same directory with a `_s` suffix, and you can verify this worked properly by running:\\n\\n```bash\\nwash claims inspect target/wasm32-unknown-unknown/release/hello_s.wasm\\n```\\n\\nYour output should be something like this, just with different `Account` and `Module` keys\\n\\n```plain\\n                              Hello - Module\\n  Account       ABYFZKXEHQWJIMBKVAVG3Y5LGEBT3MQXRYVTQBF7RVHUIG62LUK3N5EQ\\n  Module        MAMP52XKSBHNDMWK4OR4BZVBDQNNZQ5FXDXUAX7KIT7KNOKK2N3CCLZ2\\n  Expires                                                          never\\n  Can Be Used                                                immediately\\n  Version                                                       None (0)\\n  Call Alias                                                   (Not set)\\n                               Capabilities\\n  HTTP Server\\n                                   Tags\\n  None\\n```\\n\\nNow that we\'ve built and signed your actor, let\'s push it to GitHub! To do this, you\'ll need a <u>[personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)</u> with the `write:packages` capability. Refer to the link for instructions on how to create this personal access token, and feel free to tweak the other parameters to your liking.\\n\\n![](/img/ghcr-actions/new-pat.png)\\n\\nOnce created, copy this token and store it in a safe place before moving back to your terminal.\\n\\n![](/img/ghcr-actions/pat-created.png)\\n\\nNow, time to push! We\'ll use `wash` here along with some environment variables. `WASH_REG_USER` should be set to your GitHub username, and `WASH_REG_PASSWORD` should be set to your GitHub personal access token that you created before, starting with `ghp_`\\n\\n```bash\\nexport WASH_REG_USER=<your_github_username>\\nexport WASH_REG_PASSWORD=<your_gitub_personal_access_token>\\nwash reg push ghcr.io/$WASH_REG_USER/hello:0.1.0 target/wasm32-unknown-unknown/release/hello_s.wasm\\n```\\n\\nYou should see output like the following:\\n\\n```plain\\nwash reg push ghcr.io/$WASH_REG_USER/hello:0.1.0 target/wasm32-unknown-unknown/release/hello_s.wasm\\n\\n\ud83d\udebf Successfully validated and pushed to ghcr.io/brooksmtownsend/hello:0.1.0\\n```\\n\\nNow, you can navigate to your GitHub profile and access the `Packages` tab to see your `hello` actor.\\n\\n![](/img/ghcr-actions/package.png)\\n\\nBy default, our actor package is private so that it cannot be downloaded anonymously. Any wasmCloud host can be configured with a username and password for registry authentication, and in this case you can supply your GitHub username and personal access token to authenticate and download your actor. However, this step can be missed easily, so let\'s make this actor package public so you can start it anywhere. Click on your package, then on the \\"[\u2699\ufe0f](https://emojipedia.org/gear/) Package Settings\\" sidebar. From there, scroll to the bottom and \\"Change visibility\\" to public.\\n\\n![](/img/ghcr-actions/change-visibility.png)\\n\\nNow you can download and run that actor on any wasmCloud host, which can be on Mac, Linux, Windows, or even in a browser tab! You can test connectivity at any time by running:\\n\\n```bash\\nwash claims inspect ghcr.io/<your_github_username>/hello:0.1.0\\n```\\n\\n## Continuous integration with a GitHub repository\\n\\nNow that we have a Package set up for our actor, the next step is to connect it to a repository. Once we do that we can take advantage of the built-in Actions that are provided with an actor project to automatically build, test, and release actors.\\n\\nFirst step is to create a GitHub repository. For simplicity, let\'s call it `hello` to match the actor name.\\n\\n![](/img/ghcr-actions/create-repo.png)\\n\\nOnce that\'s created, we need to associate our local actor project with the GitHub repository. `cd hello` into your actor project if you haven\'t already, and then:\\n\\n```bash\\ngit add .\\ngit commit -m \\"initial commit\\"\\ngit branch -M main\\n# If you named your repository something else, simply replace the URL below\\ngit remote add origin https://github.com/<your_github_name>/hello.git\\ngit push -u origin main\\n```\\n\\nFor every commit after this one, and every pull request into `main` , your actor will be automatically built, checked for formatting, lints, and tested once you add unit tests and uncomment the `cargo test` step in `.github/workflows/build.yml`.\\n\\nFor the `.github/workflows/release.yml` action, we need to configure three repository secrets to properly sign and release your actor. The good news is, we already have all of these secrets, we just need to plop them in GitHub!\\n\\n![](/img/ghcr-actions/repo-secrets.png)\\n\\nUnder your repository settings, head to the `Secrets` dropdown and click `Actions` . Here we can configure the following secrets for use in this repository.\\n\\n| **name**         | **value**                                                                                                                                                                                                                                                                                   |\\n| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| WASH_ISSUER_KEY  | Can be found under `$HOME/.wash/keys` with the form of `<your_username>_account.nk`. Copy the contents of this file, a 56 character <u>[NKey](https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth)</u> starting with `SA`, into the value section. |\\n| WASH_SUBJECT_KEY | Can be found under `$HOME/.wash/keys/hello_module.nk`. Copy the contents of this file, a 56 character <u>[NKey](https://docs.nats.io/running-a-nats-service/configuration/securing_nats/auth_intro/nkey_auth)</u> starting with `SM`, into the value section.                               |\\n| WASMCLOUD_PAT    | Your personal access token that you previously used for `WASH_REG_PASSWORD`                                                                                                                                                                                                                 |\\n\\n![](/img/ghcr-actions/required-secrets.png)\\n\\nOnce these three secrets are configured, let\'s cut our first release of the `hello` actor. Head back to your command line and create a tag for `v0.1.0` and push it to `main`\\n\\n```bash\\ngit tag -a v0.1.0 -m \\"initial release for hello actor\\"\\ngit push -u origin v0.1.0\\n```\\n\\nThis will automatically kick off the release action, which includes building and testing the actor just like the build action, and after a few minutes you\'ll see a new GitHub release created with the OCI URL for the actor and claims information, as well as the `hello` package associated with this repo.\\n\\n![](/img/ghcr-actions/release.png)\\n\\n## Wrap up\\n\\nToday we walked through the setup process to configure a GitHub repository to automatically build, test, and release wasmCloud actors to GitHub Packages. You can continue to add to these base workflow templates to include your own custom checks as well.\\n\\n## What\'s next?\\n\\nNow that your `hello` actor is published to a public OCI registry, you can follow our <u>[run actor](https://wasmcloud.dev/app-dev/create-actor/run/)</u> tutorial with the OCI reference instead of uploading a local file. If you\'re new to wasmCloud, check out our documentation for helpful information about <u>[actors](https://wasmcloud.dev/reference/host-runtime/actors/)</u>, our <u>[security model](https://wasmcloud.dev/reference/host-runtime/security/)</u>, and more. Feedback on this process is welcome and encouraged on our <u>[project-templates](https://github.com/wasmcloud/project-templates)</u> repo; and to stay involved you can join our <u>[slack](https://slack.wasmcloud.com/)</u>."}]}')}}]);