---
title: "Deploying wasmCloud on Kubernetes"
description: "Integrating wasmCloud with Kubernetes"
sidebar_position: 1
---

wasmCloud is **compatible with, but not dependent on**, Kubernetes. We think the future of WebAssembly is bright, _and_ we also know there are plenty of systems already running in Kubernetes. For this reason, we provide the wasmCloud operator to help users run wasmCloud hosts on a Kubernetes cluster&mdash;and thereby run WebAssembly components on Kubernetes.

For high-level discussion of our approach to Kubernetes and compatibility with common cloud native tooling, see [wasmCloud on Kubernetes](/docs/1.0/kubernetes/).

On **this** page, you can find documentation for operators wishing to deploy wasmCloud with Kubernetes, including details on prerequisites such as NATS and how to deploy the operator. 

## Getting started with the wasmCloud operator

The wasmCloud operator can be found on GitHub at [https://github.com/wasmCloud/wasmcloud-operator](https://github.com/wasmCloud/wasmcloud-operator). It uses custom resource definitions (CRDs) to define wasmCloud hosts as Kubernetes resources. 

The `WasmCloudHostConfig` CRD describes the desired state of a set of wasmCloud hosts connected to the same lattice:

```yaml
apiVersion: k8s.wasmcloud.dev/v1alpha1
kind: WasmCloudHostConfig
metadata:
  name: my-wasmcloud-cluster
spec:
  # The number of wasmCloud host pods to run
  hostReplicas: 2
  # The cluster issuers to use for each host
  issuers:
    # This needs to be the public key of a cluster seed generated by
    # `wash keys gen cluster`
    - CDK...
  # The lattice to connect the hosts to
  lattice: 83a5b52e-17cf-4080-bac8-f844099f142e
  # Additional labels to apply to the host other than the defaults set in the operator
  hostLabels:
    some-label: value
  # Which wasmCloud version to use
  version: 0.82.0
  # The name of a secret in the same namespace that provides the required secrets.
  secretName: cluster-secrets
  # Enable the following to run the wasmCloud hosts as a DaemonSet
  #daemonset: true
  # The name of the image pull secret to use with wasmCloud hosts so that they
  # can authenticate to a private registry to pull components.
  # registryCredentialsSecret: my-registry-secret
```

The CRD requires a Kubernetes Secret with the following keys:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-wasmcloud-cluster
data:
  # You can generate this with wash:
  # `wash keys gen cluster`
  WASMCLOUD_CLUSTER_SEED: <seed>
  # Only required if using a NATS creds file
  # nats.creds: <base64 encoded creds file>
```
:::info[Secrets required]
The operator will fail to provision the wasmCloud Deployment if any of these secrets are missing.
:::

### Image pull secrets

You can also specify an image pull secret to use use with the wasmCloud hosts so that they can pull components from a private registry. This secret needs to be in the same namespace as the WasmCloudHostConfig CRD and must be a
`kubernetes.io/dockerconfigjson` type secret. See the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials) for more information on how to provision that secret.

Once it is created, you can reference an image pull secret in the `WasmCloudHostConfig` CRD by
setting the `registryCredentialsSecret` field to the name of the secret.

### Deploying the operator

A wasmCloud cluster requires a few things to run:

- A NATS cluster with Jetstream enabled
- WADM connected to the NATS cluster in order to support applications

If you are running locally, you can use the following commands to start a
NATS cluster and WADM in your Kubernetes cluster.

#### Running NATS

Use the upstream NATS Helm chart to start a cluster with the following
values.yaml file:

```yaml
config:
  cluster:
    enabled: true
    replicas: 3
  leafnodes:
    enabled: true
  jetstream:
    enabled: true
    fileStore:
      pvc:
        size: 10Gi
    merge:
      domain: default
```

```shell
helm upgrade --install -f values.yaml nats-cluster nats/nats
```

#### Running WADM

WADM can be run as a standalone binary or as a container. The following
command will start WADM as a Kubernetes deployment:

```shell
```

#### Start the operator

```sh
kubectl kustomize build deploy/local | kubectl apply -f -
```

### Argo CD Health Check

Argo CD provides a way to define a [custom health
check](https://argo-cd.readthedocs.io/en/stable/operator-manual/health/#custom-health-checks)
that it then runs against a given resource to determine whether or not the
resource is in healthy state.

For this purpose, we specifically expose a `status.phase` field, which exposes
the underlying status information from wadm.

With the following ConfigMap, a custom health check can be added to an existing
Argo CD installation for tracking the health of wadm applications.

```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-cm
  namespace: argocd
  labels:
    app.kubernetes.io/name: argocd-cm
    app.kubernetes.io/part-of: argocd
data:
  resource.customizations: |
    core.oam.dev/Application:
      health.lua: |
        hs = {}
        hs.status = "Progressing"
        hs.message = "Reconciling application state"
        if obj.status ~= nil and obj.status.phase ~= nil then
          if obj.status.phase == "Deployed" then
            hs.status = "Healthy"
            hs.message = "Application is ready"
          end
          if obj.status.phase == "Reconciling" then
            hs.status = "Progressing"
            hs.message = "Application has been deployed"
          end
          if obj.status.phase == "Failed" then
            hs.status = "Degraded"
            hs.message = "Application failed to deploy"
          end
          if obj.status.phase == "Undeployed" then
            hs.status = "Suspended"
            hs.message = "Application is undeployed"
          end
        end
        return hs
```

### Testing

Make sure you have a Kubernetes cluster running locally. Some good options include [Kind](https://kind.sigs.k8s.io/) or Docker Desktop.